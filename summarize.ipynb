{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Summarization\n",
    "### Umema Ashar 22i-2036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code extracts the dataset from the PubMed_summarization dataset from HuggingFace\n",
    "\n",
    "1- each article is preprocessed\n",
    "\n",
    "2- extractive summarization is performed on each article\n",
    "\n",
    "3- image is generated\n",
    "\n",
    "4- all summaries are stored in a text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Define stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "    \n",
    "    # Remove stopwords, punctuation, and numbers\n",
    "    tokens = [word for word in tokens if word not in stop_words and not re.search(r'\\d', word)]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_text(text):\n",
    "    image_size = (400, 300) \n",
    "    image = np.ones(shape=(image_size[1], image_size[0], 3), dtype=np.uint8) * 255  # White image\n",
    "    image = Image.fromarray(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summarization(article, summary_style='brief'):\n",
    "    sentences = sent_tokenize(article)\n",
    "    words = preprocess(article)\n",
    "    word_freq = Counter(words)\n",
    "    sentence_scores = {sent: sum(word_freq[word] for word in preprocess(sent)) for sent in sentences}\n",
    "    top_n = 3 if summary_style == 'brief' else 5\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:top_n]\n",
    "    return ' '.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_summary_and_image(article):\n",
    "    summary_style = 'brief'  # You can change this to 'detailed' if needed\n",
    "    summary = extractive_summarization(article, summary_style)\n",
    "    generated_image = generate_image_from_text(article)  # Generate image based on text\n",
    "    return summary, generated_image\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    summaries = []\n",
    "    for example in dataset:\n",
    "        article = example['article']\n",
    "        summary, _ = generate_summary_and_image(article)\n",
    "        summaries.append((article, summary))\n",
    "    \n",
    "    # Write summaries to a text file\n",
    "    output_file = f\"summaries_{int(time.time())}.txt\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for idx, (article, summary) in enumerate(summaries, start=1):\n",
    "            outfile.write(f\"Article {idx}:\\n{article}\\n\\nSummary:\\n{summary}\\n\\n\")\n",
    "\n",
    "    print(f\"Summaries saved successfully to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"ccdv/pubmed-summarization\", split=\"train\")\n",
    "    process_dataset(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
